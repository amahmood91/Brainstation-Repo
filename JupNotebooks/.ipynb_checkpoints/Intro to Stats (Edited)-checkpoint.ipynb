{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "enD6N3SDqJ0M"
   },
   "source": [
    "# Probability and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUXSs93HqJ0O"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSuww-W6qJ0R"
   },
   "source": [
    "In this unit, we're going to review the statistics we learned in the prep course, and then learn more advanced techniques and terms.\n",
    "\n",
    "\n",
    "## Statistics\n",
    "\n",
    "### Single Variable Descriptive Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Moavcf4YqJ0S"
   },
   "source": [
    "**Central Tendency**\n",
    "\n",
    "*  **Mean**:  this is the term for what we generally call an \"average\".  We add all of the numbers in our data set, and then divide by the size of the set.  Symbolically, if $A = \\{a_1,a_2,\\ldots,a_n\\}$ is our data set, then the arithmetic mean (denoted $\\bar{A}$) is computed as \n",
    "$$ \\begin{align}\n",
    "\\bar{A} &=  \\frac{1}{n}(a_1+a_2+\\ldots+a_n) \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^na_i\n",
    "\\end{align}$$\n",
    "* **Median**: for a given data set (of numbers), the median is the number that separates the top half of the data from the bottom half.  If the size of the data set is even )so there is no single data point in the middle), the median is given by the mean of the two middle values, after arranging the data into ascending order.\n",
    "* **Mode**: the mode is the number that occurs most frequently in a data set.\n",
    "\n",
    "**Dispersion**\n",
    "\n",
    "* **Biased Variance**: The _variance_ of a dataset is the average of the squared distances from the mean.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}(X) &= \\frac{1}{n}\\left((x_1-\\bar{X})^2+(x_2-\\bar{X})^2+\\ldots+(x_n-\\bar{X})^2\\right) \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n (x_i-\\bar{X})^2\n",
    "\\end{align}$$  In other words, we take the arithmetic mean of the * squared-difference between the data set elements and the set's mean*.\n",
    "* **Unbiased Variance**:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Var}(X) &= \\frac{1}{n-1}\\left((x_1-\\bar{X})^2+(x_2-\\bar{X})^2+\\ldots+(x_n-\\bar{X})^2\\right) \\\\\n",
    "&= \\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{X})^2\n",
    "\\end{align}$$\n",
    "* **Standard Deviation**: the standard deviation is the square root of the variance.  One useful property of the standard deviation is that it is in the same units as the original data set (unlike the variance).  A low standard deviation implies the data set points are clustered near the mean, whereas a high standard deviation implies the data set points tend to be further from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3I2FrBLAqJ0T"
   },
   "source": [
    "We can use built-in functions to calculate statistics of Series objects that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "267EFqPhqJ0T",
    "outputId": "62064b7e-4f2f-424f-ab95-d0b96b5cb52c"
   },
   "outputs": [],
   "source": [
    "s = pd.Series([1, 2, 3, 4, 5])\n",
    "print(s.mean())\n",
    "print(s.median())\n",
    "print(s.std())\n",
    "print(s.var())\n",
    "\n",
    "print('numpy')\n",
    "print(np.mean(s))\n",
    "print(np.median(s))\n",
    "print(np.std(s))\n",
    "print(np.var(s))\n",
    "\n",
    "#in np, it uses a varaince calculation that does 1/(n-1) (aka unbiased variance) instead of 1/n in the mean calculation in the base function (biased variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrfZkPPrqJ0Z"
   },
   "source": [
    "Let's practice using the Anscombe's quartet dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AItN9xzPqJ0Z"
   },
   "outputs": [],
   "source": [
    "anscombe = pd.read_csv('data/anscombe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kO1rluKtqJ0b",
    "outputId": "af9a6bff-d9aa-4438-b990-2e648861cdc3"
   },
   "outputs": [],
   "source": [
    "anscombe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anscombe.info()\n",
    "\n",
    "anscombe.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01gO55GOqJ0e"
   },
   "source": [
    "Recall that Anscombe's quartet is four datasets with very similar descriptive statistics. We'll plot the four below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "899W0FwLqJ0f",
    "outputId": "4b8aca93-eaf2-4c59-d6b7-022d87c40684"
   },
   "outputs": [],
   "source": [
    "anscombe[anscombe['Dataset'] == \"I\"].plot.scatter(x='X', y='Y')\n",
    "anscombe[anscombe['Dataset'] == \"II\"].plot.scatter(x='X', y='Y')\n",
    "anscombe[anscombe['Dataset'] == \"III\"].plot.scatter(x='X', y='Y')\n",
    "anscombe[anscombe['Dataset'] == \"IV\"].plot.scatter(x='X', y='Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QHqHn6TdqJ0i"
   },
   "source": [
    "Verify which descriptive statistics are equal across each of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bYUC7lTsqJ0j",
    "outputId": "59c8dd7d-c5ad-4f66-b516-2e73473a82e3"
   },
   "outputs": [],
   "source": [
    "# Solution\n",
    "# The datasets have the same means, standard deviations, and variances in X\n",
    "anscombe.groupby('Dataset').agg(['sum','mean', 'median', 'std', 'var'])\n",
    "\n",
    "#WE SEE THAT THESE DESCRIPTIVE STATS ARE THE SAME FOR EACH SUBSET, HOWEVER THE PLOTS LOOK VERY DIFFERENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1MeziIe7qJ0n"
   },
   "source": [
    "### Covariance and Correlation\n",
    "\n",
    "The descriptive statistics that we've studied so far give us ways to summarise aspects of a single column of data. _Covariance_ and _correlation_ are ways to summarise how two columns of data interact with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOoGGrUiqJ0z"
   },
   "source": [
    "#### The Covariance Matrix\n",
    "The covariance is defined pairwise -- that is, there is only a covariance between _pairs_ of variables. To look at covariances within an entire dataset, we can use something called the _covariance matrix_. For a dataset with $n$ columns, the covariance matrix of that dataset is an $n\\times n$ matrix where the $ij$-th entry is the covariance of the $i$-th and $j$-th columns. Pandas gives you a covariance matrix of a dataframe using the `df.cov()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SK8nVzphqJ00",
    "outputId": "df72e5eb-e03d-4698-e204-6fdc0b648bba"
   },
   "outputs": [],
   "source": [
    "#anscombe.loc[anscombe['Dataset'] == 'I', ['X', 'Y']].cov()\n",
    "\n",
    "print('Dataset I')\n",
    "print(anscombe.loc[anscombe['Dataset'] == 'I', ['X', 'Y']].cov())\n",
    "\n",
    "print('Dataset II')\n",
    "print(anscombe.loc[anscombe['Dataset'] == 'II', ['X', 'Y']].cov())\n",
    "\n",
    "print('Dataset III')\n",
    "print(anscombe.loc[anscombe['Dataset'] == 'III', ['X', 'Y']].cov())\n",
    "\n",
    "print('Dataset IV')\n",
    "print(anscombe.loc[anscombe['Dataset'] == 'IV', ['X', 'Y']].cov())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GCuKh325qJ03"
   },
   "source": [
    "**Question:** What's along the diagonals? \n",
    "\n",
    "**Problem:** Check the covariance of X and Y for each dataset in Anscombe's quartet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9b0X9SXqJ03",
    "outputId": "006d8df6-9959-4fb6-fb68-cad7bb729533"
   },
   "outputs": [],
   "source": [
    "# one option\n",
    "anscombe.groupby(\"Dataset\").agg(\"cov\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3Q8p0H1qJ07"
   },
   "source": [
    "### Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBvXcms6qJ08"
   },
   "source": [
    "When $X$ and $Y$ move together, then $\\text{Cov}(X,Y) \\gg 0$, and when they move in opposite directions then $\\text{Cov}(X,Y) \\ll 0$. But there's still a problem: how big is big? It turns out that the covariance is very sensitive to units. \n",
    "\n",
    "If $X$ is measured in inches, $Y$ is measured in feet, and $Z$ is just $X$ in feet, then we would have $\\text{Cov}{Z,Y} = 12\\text{Cov}(X,Y)$, even though $Z$ and $X$ are measuring the same thing. So there's no way to know in a vaccuum when the covariance is large. One solution to this is to use a normalized version of the covariance. This is known as the Correlation, and it is computed as\n",
    "\n",
    "$\\text{Cor}(X,Y) = \\frac{\\text{Cov(X,Y)}}{\\text{SD}(X)\\text{SD}(Y)}$\n",
    "\n",
    "Sometimes this number is called Pearson's correlation coefficient, and sometimes it is denoted by the Greek letter $\\rho$, in which case we write the correlation between datasets $X$ and $Y$ as $\\rho_{XY}$.\n",
    "\n",
    "The number $\\rho_{XY}$ is always between 1 and -1. A correlation of 1 corresponds to a perfect linear relationship with a positive slope, and a correlation of -1 corresponds to a perfect linear relationship with a negative slope.\n",
    "\n",
    "\n",
    "We can define the _Correlation Matrix_ analogously to the covariance matrix. This is helpfully built for us in Pandas, and is useful to look at. Let's use it with the diamonds dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ux45rqgTqJ08",
    "outputId": "129f7e92-1c1e-47a6-a8ad-36839d4077d8"
   },
   "outputs": [],
   "source": [
    "diamonds = pd.read_csv('data/diamonds.csv')\n",
    "diamonds.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zX-FhEsgqJ0_"
   },
   "source": [
    "That's a lot of numbers to look at. Seaborn has a useful way to visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2keVA82qJ0_",
    "outputId": "e7f666d3-54e4-4422-b9ab-5ee479e2b125"
   },
   "outputs": [],
   "source": [
    "#seaborn is a plotting package that is very specific for statistical graphs, comapred to matplotlib which is a more generic plotting package\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(diamonds.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7Qo2arTqJ1B"
   },
   "source": [
    "We notice that the diagonal entries are all 1, because each column is maximally correlated with itself.  The correlation between `price` and `carat` is ~0.923, which is relatively close to 1.  This means the values in the columns tend to move together (i.e.: there is a strong linear relation between price and carat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample vs. Population\n",
    "\n",
    "In general, when analyzing data we want to get an understanding of either the population that our sample comes from, or the process which created the data.\n",
    "\n",
    "It's nice to know that our sales over time are increasing, but if we can infer a cause, we can ensure that it continues to happen.\n",
    "\n",
    "\n",
    "\n",
    "A population can have an underlying distribution and we generally want to know what the parameters of that distribution are. \n",
    "\n",
    "For example: We want to know what the average height of people living in Toronto. \n",
    "\n",
    "1. We take a sample of the population (all people living in Toronto). \n",
    "2. We can calculate the average height for our sample. This calculated average is now the statistic that estimates the population parameter, $\\mu$.\n",
    "\n",
    "\n",
    "\n",
    "Let's think about rolling some dice, which we can do in numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1234) #this makes sure we get the same numbers!\n",
    "possiblerolls = list(range(1,7))\n",
    "print(possiblerolls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to **estimate the sum of all dice rolled at one time**. If we roll two dice, we can calculate the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(possiblerolls) + np.random.choice(possiblerolls) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If presented with only one or two rolls, it is nearly impossible to understand the way this data was generated. However, if we repeat the process many times, it quickly becomes easy to see how the outcomes are distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls = np.random.choice(possiblerolls, replace = True, size = (50000,2))\n",
    "print(rolls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(rolls.sum(axis = 1), bins = 11);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rolls.sum(axis = 1)) #axis = 1 means sum by row - aka sum all the columns per row\n",
    "print(rolls.sum(axis = 0)) #axes = 0 means sum by column - aka sum all rows per column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the number of dice, the distribution becomes a familiar shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls = np.random.choice(possiblerolls, replace = True, size = (50000,2000))\n",
    "print(rolls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(rolls.sum(axis = 1), bins = 599, range = (6700, 7300));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hopefully the above plot is somewhat familiar to you. We have generated something that approximates the 'Normal Distribution'.\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = rolls.sum(axis = 1).mean()\n",
    "std = rolls.sum(axis = 1).std()\n",
    "print(mean)\n",
    "print(std)\n",
    "\n",
    "from scipy.stats import norm\n",
    "# arange function starts at 6700, ends at 7300, increments by 20\n",
    "x = np.arange(6700, 7300, 20)\n",
    "#print(x)\n",
    "\n",
    "plt.plot(x, norm.pdf(x, mean, std));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Not exactly the same, but we would expect the approximation to improve as we increase the number of dice.\n",
    "\n",
    "### Central Limit Theorem\n",
    "\n",
    "The central limit theorem states that as we increase the number of samples from a population, the mean of the samples approaches the mean of the population, with normally distributed error.\n",
    "\n",
    "Let's be a little more strict about our approach to the central limit theorem.\n",
    "\n",
    "We expect for each dice roll, to have a mean value of (1 + 2 + 3 + 4 + 5 + 6)/6 = 3.5:\n",
    "\n",
    "$$ \\mu = \\frac{1}{n}\\sum_{i=1}^nx_i $$\n",
    "\n",
    "Our expected variance is therefore the sum of ((3.5-1)^2 + (3.5 - 2)^2.....)/6:\n",
    "\n",
    "\n",
    "$$ \\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i-\\mu)^2 $$\n",
    "\n",
    "Which gives us a total variance of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate mean manually\n",
    "die_mean = np.arange(1,7).sum()/6\n",
    "print(die_mean)\n",
    "\n",
    "#calculate variance\n",
    "sum((np.arange(1,7)-die_mean)**2)/6\n",
    "\n",
    "#sum((np.arange(1,7) - 3.5)**2)/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our larger sample rolls, we would expect the CLT to be pretty true - as we rolled 2000 dice 50000 times, to get our expected value, µ, we multiply our mean by 2000: 7000. We have an expected variance in this value of 2000 * 2.916: 5833.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls.sum(axis = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolls.sum(axis = 1).var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad! Let's take a look at some smaller repeats and see how they fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in [1, 5 , 10, 100, 1000]:\n",
    "    rolls = np.random.choice(possiblerolls, replace = True, size = (i,2000))\n",
    "    print(f\"mean of {i} rolls: {rolls.sum(axis = 1).mean()}\")\n",
    "    print(f\"var of {i} rolls: {rolls.sum(axis = 1).var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have less rolls per repeat? With 20 rolls, the expected value of the parameter µ, is 20 * 3.5 = 70. \n",
    "\n",
    "We have an expected variance in this value of 20 * 2.916 = 58.32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1, 5 , 10, 100, 1000]:\n",
    "    rolls = np.random.choice(possiblerolls, replace = True, size = (i,20))\n",
    "    print(f\"mean of {i} rolls: {rolls.sum(axis = 1).mean()}\")\n",
    "    print(f\"var of {i} rolls: {rolls.sum(axis = 1).var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and figure out the underlying mean by just sampling. This is similar to how we work in statistics, we either don't know the underlying mechanism of how the data is generated, or we can't ask everyone who they want to vote for. Instead, we take as large a sample as possible, and use it to approximate the population.\n",
    "\n",
    "So, we can have five dice this time. Let's try with varying numbers of repeats, and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = []\n",
    "for i in [1, 5, 10, 100, 1000]:\n",
    "    meanrolls = np.random.choice(possiblerolls, replace = True, size = (i,5)).sum(axis =1)\n",
    "    mydata.append(meanrolls)\n",
    "\n",
    "fig = plt.figure(figsize=(15.0, 3.0))\n",
    "\n",
    "#enumerate gives index i for value j in the list\n",
    "for i,j in enumerate([1, 5, 10, 100, 1000]):\n",
    "    axes = fig.add_subplot(1, 5, i+1)\n",
    "    axes.hist(mydata[i], range = (5,30))\n",
    "    axes.set_title(f'{j} repeats')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Normal Distribution\n",
    "\n",
    "We have previously seen that as we increase the number of samples, we approach the normal, or gaussian distribution (aka the bell curve).\n",
    "\n",
    "The shape of the distribution is based on the mean and variance:\n",
    "\n",
    "$$ f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2} $$\n",
    "\n",
    "Let's take a look:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best things about working in python, numpy and scipy is that a lot of it is baked in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0\n",
    "var = 1\n",
    "xvals = np.arange(-3, 3, 0.01)\n",
    "\n",
    "plt.plot(xvals, norm.pdf(xvals, mean, var));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some nice properties of the Normal distribution. If we know (or can assume) that a population is normally distributed, we can figure out how 'likely' a value is to have come from the distribution.\n",
    "\n",
    "We can call this a Standard Score, or Z-Score.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/a9/Empirical_Rule.PNG\" height = \"600\" width = \"600\">\n",
    "\n",
    "A Z-Score is Calculated using the formula:\n",
    "\n",
    "$$ z =  \\frac{x - \\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we got a roll of 29 from a roll of 6 dice, if we assume that the totals are normally distributed, what is our Z-Score? The $$\\sigma$$ here refers to our standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 3.5 * 6 #population of 6 so multiply by 6 to get population mean\n",
    "var = np.sum(((3.5 - np.arange(1,7))**2)/6*6) #mult by 6 to get pop variance\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "print(mean, var, sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(29 - mean)/sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean+(sd*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how likely was this to happen? We can look up the cumulative distribution function for the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm.cdf((29 - mean)/sd) #prob of getting a roll of less than or equal to 29\n",
    "1-norm.cdf((29 - mean)/sd) #prob of getting atleast a roll of 29\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would expect a lower value about 97.2% of the time. Obviously, we know that the outcome of 6 dice is not completely normally distributed. We would want to repeat our rolls to be able to make inference about it.\n",
    "\n",
    "Right now we only have one sample. We know that as we increase the number of repeats in our samples, we approach the mean of the population, we can use this to alter our sd in the formula:\n",
    "\n",
    "$$ z =  \\frac{x - \\mu}{SEM} $$\n",
    "\n",
    "$$ SEM = \\frac{\\sigma}{\\sqrt n} $$\n",
    "\n",
    "Where SEM is the standard deviation of our population, divide by the number of samples in our test set.\n",
    "\n",
    "Imagine we now have 3 samples: `[29, 28, 27]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([27,28,29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEM = sd/np.sqrt(3)\n",
    "\n",
    "zscore = (np.mean([27,28,29]) - mean)/SEM\n",
    "norm.cdf(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We have some bike crash data on one street from the city of Toronto: we want to know if the year 2016 was an outlier, or normal year.\n",
    "\n",
    "The data for 1985-2015 are as below:\n",
    "\n",
    "[12, 30, 14, 66, 76, 65, 47, 78, 25, 96, 17, 42, 29, 20, 34, 39, 65,\n",
    "       13, 91, 62, 22,  8, 57, 28, 71, 75,  8, 48, 21, 15, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Calculate the mean and standard deviation of this data\n",
    "2. 2016 had 115 crashes - calculate the z-score\n",
    "3. Write a function to calculate the zscore, without using numpy or scipys mean, std or z functions.\n",
    "4. Use google to figure out how to go from a zscore to a 'p-value'. Can you interpret this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#1. Mean: 43.67741935483871, Variance (biased): 702.0249739854319, Std Dev: 26.49575388596127\n",
      "\n",
      "#2. Z score: 2.691849454525294\n",
      "\n",
      "#3. Manually calculated z-score: 2.691849454525294\n",
      "\n",
      "#4. P value: 0.9964471493822556\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_data = pd.Series([12, 30, 14, 66, 76, 65, 47, 78, 25, 96, 17, 42, 29, 20, 34, 39, 65, 13, 91, 62, 22, 8, 57, 28, 71, 75, 8, 48, 21, 15, 80])\n",
    "\n",
    "#1. Calculate mean, variance and std dev\n",
    "mean_crashes = np.mean(bike_data)\n",
    "var_crashes = np.var(bike_data)\n",
    "#sd_crashes = np.sqrt(var_crashes)\n",
    "sd_crashes = np.std(bike_data)\n",
    "\n",
    "print(f\"#1. Mean: {mean_crashes}, Variance (biased): {var_crashes}, Std Dev: {sd_crashes}\\n\")\n",
    "\n",
    "# 2. Calculate z-score of 115 crashes\n",
    "x_val = 115\n",
    "z_value = (x_val - mean_crashes)/sd_crashes\n",
    "print(f\"#2. Z score: {z_value}\\n\")\n",
    "\n",
    "# 3. Make a z-score calculating function \n",
    "\n",
    "#function 1\n",
    "\n",
    "def calc_z_score(data, sample_value):\n",
    "    mu_data = sum(data)/len(data)\n",
    "    \n",
    "    diff_data = []\n",
    "    for i in data:\n",
    "        diff = (i - mu_data)**2\n",
    "        diff_data.append(diff)\n",
    "            \n",
    "    sigma = (sum(diff_data)/len(data))**(1/2)\n",
    "    \n",
    "    z_score = (sample_value-mu_data)/sigma\n",
    "    return(z_score)\n",
    "\n",
    "print(f\"#3. Manually calculated z-score: {calc_z_score(bike_data, 115)}\\n\")\n",
    "\n",
    "#4 Get p-value from z-score -- NEEDS TO BE CONFIRMED\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "z_score = calc_z_score(bike_data, 115)\n",
    "\n",
    "p_val = st.norm.cdf(z_score)\n",
    "print(f\"#4. P value: {p_val}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Unit 06 - Statistics and Probability.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
