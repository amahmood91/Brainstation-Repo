{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Statistics\n",
    "\n",
    "If you dive deeply into the way people talk about statistics, sooner or later you will find people discussing\n",
    "'Frequentists vs Bayesians'. So far, we have only talked about frequentist statistics - how to carry out hypothesis testing, and calculate P-values. \n",
    "\n",
    "In frequentist statistics, we are saying that there is some ground truth in the thing we are measuring - the mean, sd or different between treatments. As we increase our sample sizes, we gain better estimates into the 'truth' using our techniques that we have covered.\n",
    "\n",
    "Bayesians have a different intepretation of probability - we update our expectations based on the data, and try and form the most reasonable expectation.\n",
    "\n",
    "The following cartoon is an example of frequentist statistics [taken to its extreme](https://xkcd.com/1132/).\n",
    "\n",
    "In general, most people are probably Bayesian by default - if we flip a coin ten times, and get 8 heads, we dont conclude that the coin is biased (though a frequentist would), we have a strong 'prior' that a random coin is not biased, and thus we were just lucky (or unlucky).\n",
    "\n",
    "A major difference is the presence of priors in Bayesian reasoning - in the cartoon, we have an incredibly strong prior that the sun has not exploded, so reject the resoning of the frequentist. In the event that the neutrino detecor was not hooked up to the die roll, and it reported the sun had exploded, we would probably just conclude that the detector is broken, rather than the sun exploding.\n",
    "\n",
    "### Bayes Theorem\n",
    "\n",
    "Let's take an example - We have a test for drug screening police officers on duty for a specific drug. We know that if an officer has been using the drug, 99% of the time, the test will be positive. Similarly, if the officer has not been using the drug, 99% of the time the test will be negative. We know that 0.5% of the police force are using the drug.\n",
    "\n",
    "If we score an officer as positive on the test, what is the chance that the officer has been taking the drug?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show of hands for 99%, 50-99%, 10-50%, less than 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes rule is notoriously counter-intuitive when posed with problems like this.\n",
    "\n",
    "Let's work through the formula:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "In this case, $P(A|B)$ denotes the probability of being a user (A), given a positive test (B).\n",
    "\n",
    "$P(B|A)$ is the probability of a positive test, given a that the subject is a user, $P(A)$ is the probability of being a user.\n",
    "\n",
    "$P(B)$ is the probability of a positive test - In our case, it is the probability of a user scoring a positive, multiplied by the probability of being a user, plus the probability of a positive test for a non-user, multiplied by the probability of not being a user.\n",
    "\n",
    "Let's do the math:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33221476510067094\n"
     ]
    }
   ],
   "source": [
    "p_user = 0.005\n",
    "p_nonuser = 1 - p_user\n",
    "p_user_pos = 0.99\n",
    "p_nonuser_pos = 1 - 0.99\n",
    "\n",
    "prob = (p_user_pos * p_user)/(p_user_pos * p_user + p_nonuser_pos * p_nonuser)\n",
    "\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this is a problem! If we had simply done the test and told people that they were 99% liklely to have taken the drug, we would be completely wrong.\n",
    "\n",
    "What we have done, is used a prior (in this case the p(user)) to make sure that we are sensible about interpreting our test. We know that drug use in rare in our population, so the majority of positive tests come from false positives.\n",
    "\n",
    "Sensitivity and Specificity are the terms commonly used in medical diagnosis - in the above example our sensitivity is the probability that we detect a true drug user (0.99), and the specificity is the probability of correctly identifying a non-user (also 0.99 in our example, but not necessarily even). The  rarity of a disease will often cause false positives - you can imagine even if we have a test for a disease which is 99% for both specificity and sensitivity if the disease is rare, the majority of positives will be errors.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "When the details of the NSAs surveillance programs were leaked a few years back, the criticisms mainly came from civil  liberties - the government shouldn't be spying on citizens.\n",
    "\n",
    "Let's take a different approach, and see if it would even work, if we accepted the premise that the surveillance is necessary.\n",
    "\n",
    "Let's set the proportion of terrorists at 1/1,000,000 of the population - this is our prior. Whether or not this is reasonable, it seems at around the correct kind of size. This is often the strongest reasoning we will have to give for a prior!\n",
    "\n",
    "We have a team of NSA data scientists create a model on the data, which has a 95% chance of detecting a terrorist (sensitivity), and 0.2% chance of a false positive: saying someone is a terrorist, when they are not. \n",
    "\n",
    "1. What is the probability that someone identified as a terrorist by the NSA is a real terrorist?\n",
    "2. Is this a useful model? How many false positives do we expect?\n",
    "3. What recommendations do you have for the modeller - should they focus on false positives, faslse negatives, or give up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00047477495667053816\n"
     ]
    }
   ],
   "source": [
    "p_terrorist = 1/1000000\n",
    "p_identified_terrorist = 0.95\n",
    "p_identified = 0.002*(999999/1000000) + 0.95*(1/1000000)\n",
    "\n",
    "p_terrorist_identified = p_identified_terrorist*p_terrorist / p_identified\n",
    "\n",
    "print(p_terrorist_identified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "\n",
    "Naive Bayes is a set of algorithms for classifying data based on *features*. In the simplest case, given a set of data, we can decide which class it belongs to based on its attributes. you can imagine the model looking at mammals vs birds, and finding that the $P(bird|wings)$ is high, with some $(mammal|wings)$ from bats. It will carry out the probability for each independant variable we give, and then find the most likely class.\n",
    "\n",
    "The Naive Bayes classifier determines what *class* a given set of data belongs to, given previous data it has trained the model on.  Suppose we have many classes $C_1,C_2,\\ldots,C_n$, and we represent the set of data to be classified as $\\textbf{x} =  [x_1, x_2, \\cdots , x_k]$.  The probability that the given data $\\textbf{x}$ belongs to class $C_i$ is given by\n",
    "\n",
    "$$ P(C_i\\,|\\,\\textbf{x}) = \\frac{P(C_i)P(\\textbf{x}\\,|\\,C_i)}{P(\\textbf{x})}$$\n",
    "\n",
    "We will carry out a few simple examples - Naive Bayes is often classified as a machine learning method, which we will be talking about in a couple weeks time.\n",
    "\n",
    "You can read the documentation here:\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "Let's use an example dataset, to cluster fruits versus vegetables:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Used in salads</th>\n",
       "      <th>Grows Underground</th>\n",
       "      <th>Served cooked</th>\n",
       "      <th>Needs Peeling</th>\n",
       "      <th>Fruit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Used in salads  Grows Underground  Served cooked  Needs Peeling  Fruit\n",
       "0               0                  0              0              0      1\n",
       "1               1                  0              1              0      1\n",
       "2               0                  1              1              1      0\n",
       "3               1                  1              0              1      0\n",
       "4               0                  0              0              1      1\n",
       "5               0                  1              1              1      0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# data:\n",
    "columns = ['Used in salads', 'Grows Underground', 'Served cooked', 'Needs Peeling', 'Fruit']\n",
    "\n",
    "apple = [0,0,0,0,1]\n",
    "tomato = [1,0,1,0,1]\n",
    "potato = [0,1,1,1,0]\n",
    "carrot = [1,1,0,1,0]\n",
    "banana = [0,0,0,1,1]\n",
    "turnip = [0,1,1,1,0]\n",
    "\n",
    "data = pd.DataFrame([apple, tomato, potato, carrot, banana, turnip], columns = columns)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from here, we will use the classifier to figure out the probabilities - under the hood, here is effectively what is happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: array([1, 3, 2, 3]), 1: array([1, 0, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values\n",
    "\n",
    "counts = {}\n",
    "for label in np.unique(y):\n",
    "    counts[label] = x[y == label].sum(axis = 0)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier then compares the frequency of each feature of x to each label in y, and calculates a probability that a new item with that attribute belongs to the class.\n",
    "\n",
    "We use BernoulliNB here, as we have all 1/0 data - GaussianNB stores mean and sd of each value for a continuous var, and Mulitnomial allows multiple categorical values. When we see new data, we use the rule to classify it into the new bin:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25 0.75]]\n",
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 3., 2., 3.],\n",
       "       [1., 0., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "nbmodel = BernoulliNB()\n",
    "nbmodel.fit(x,y)\n",
    "\n",
    "print(nbmodel.predict_proba(np.array([1,0,0,1]).reshape(1, -1))) #a pineapple, probability\n",
    "print(nbmodel.predict(np.array([1,0,0,1]).reshape(1, -1))) #just predict\n",
    "\n",
    "nbmodel.feature_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create and fit a Gaussian Naive Bayes for the following data. Leave the animal names out of the model matrix.\n",
    "\n",
    "| bird        | Animal  Name         | size(cm)  |  wings  | eggs | aquatic |\n",
    "| ------------- |:-------------:| -----| --- | --- | --- |\n",
    "| 0      | bat | 10 |1  | 0 | 0 |\n",
    "| 0      | rat | 10 |0  | 0 | 0 |\n",
    "| 0 | flying lizard |5  | 1 | 1 | 0|\n",
    "| 1 | penguin |20  | 1 | 1 | 1|\n",
    "| 1 | robin | 10 | 1| 1| 0|\n",
    "| 1 | pigeon | 15 | 1| 1| 0|\n",
    "| 1 | emu | 150 | 1| 1| 0|\n",
    "| 0 | fish | 20 | 0 | 1 | 1 |\n",
    "| 0 | wasp | 1 | 1 | 1 |  0 |\n",
    "| 0 | frog | 5 | 0 | 0 | 1|\n",
    "| 1 | duck | 20 | 1 | 1 | 1|\n",
    "| 1 | heron |50 | 1 | 1 | 1|\n",
    "| 0 | pterodactyl  | 200 | 1 | 1| 0|\n",
    "\n",
    "Predict some more animals:\n",
    "\n",
    "\n",
    "* Otter\n",
    "* Flying squirrel\n",
    "* Dog\n",
    "* Swan\n",
    "* Ostrich\n",
    "* Crocodile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>flies</th>\n",
       "      <th>eggs</th>\n",
       "      <th>aquatic</th>\n",
       "      <th>is_bird</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    size  flies  eggs  aquatic  is_bird\n",
       "0     10      1     0        0        0\n",
       "1     10      0     0        0        0\n",
       "2      5      1     1        0        1\n",
       "3     20      0     1        1        1\n",
       "4     10      1     1        0        1\n",
       "5     15      1     1        0        1\n",
       "6    150      0     1        0        1\n",
       "7     20      0     1        1        0\n",
       "8      1      1     1        0        0\n",
       "9      5      0     0        1        0\n",
       "10    20      1     1        1        1\n",
       "11    50      1     1        1        1\n",
       "12   200      1     1        0        0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = ['size','flies','eggs','aquatic','is_bird']\n",
    "\n",
    "bat = [10,1,0,0,0]\n",
    "rat = [10,0,0,0,0]\n",
    "flying_lizard = [5,1,1,0,1]\n",
    "penguin = [20,0,1,1,1]\n",
    "robin = [10,1,1,0,1]\n",
    "pigeon = [15,1,1,0,1]\n",
    "emu = [150,0,1,0,1]\n",
    "fish = [20,0,1,1,0]\n",
    "wasp = [1,1,1,0,0]\n",
    "frog = [5,0,0,1,0]\n",
    "duck = [20,1,1,1,1]\n",
    "heron = [50,1,1,1,1]\n",
    "pterodactyl = [200,1,1,0,0]\n",
    "\n",
    "df = pd.DataFrame([bat,rat,flying_lizard,penguin,robin,pigeon,emu,fish,wasp,frog,duck,heron,pterodactyl], columns = columns)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41.        ,  0.5       ,  0.5       ,  0.33333333],\n",
       "       [38.57142857,  0.71428571,  1.        ,  0.42857143]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define x & y\n",
    "x = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values\n",
    "\n",
    "# fit model\n",
    "nbmodel = GaussianNB()\n",
    "nbmodel.fit(x,y)\n",
    "\n",
    "nbmodel.theta_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.   ]\n",
      " [1.    0.   ]\n",
      " [1.    0.   ]\n",
      " [0.001 0.999]\n",
      " [0.014 0.986]\n",
      " [0.009 0.991]]\n"
     ]
    }
   ],
   "source": [
    "# size, flies, eggs, aquatic\n",
    "\n",
    "otter = [30, 0, 0, 1]\n",
    "flying_squirrel = [10, 1, 0, 0]\n",
    "dog = [50, 0, 0, 0]\n",
    "swan = [40, 1, 1, 1]\n",
    "ostrich = [150, 0, 1, 0]\n",
    "crocodile = [150, 0, 1, 1]\n",
    "\n",
    "animal_list = [otter, flying_squirrel, dog, swan, ostrich, crocodile]\n",
    "\n",
    "print( np.round(nbmodel.predict_proba(animal_list),3) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Advanced Bayesian Statistics\n",
    "\n",
    "Once we get more complicated, we have a range of options in Bayesian statistics - most of the models we have used so far can occur in Bayesian versions.\n",
    "\n",
    "Let's take a look at how we might determine if a coin was biased, using a Bayesian framework.\n",
    "\n",
    "We know that a coin is expected to be 50:50 heads to tails: we have previously treated it as a random sample of a 1 or 0. Now we have seen the Bernoulli distribution, we can treat it as a Bernoulli sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 0, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.bernoulli.rvs(0.5, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine I have a coin, and you have no prior about how it will behave. In this case, you have an uniform prior along the axis 0 to 1 for the proportion of coin tosses that will be heads.\n",
    "\n",
    "We have a way of figuring out what type of distribution our prior can be based on some math, that is a bit advanced for this bootcamp (and a lot of statisticians). For now, let's just say, in the simple case that our sample is generated by a known distribution, we can find a distribution that we expect it the results of our sampling to look like. When we get more complicated is when we need to use software package to handle the math. PyMC3 is what we would normally use for Bayesian inference when needed in Python. Stan is a nice R package with similar functionality.\n",
    "\n",
    "In the case of the Bernoulli Distribution, our 'conjugate' prior is the beta distribution, with it's two parameters as 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as graph\n",
    "\n",
    "prior = stats.beta  # prior distribution\n",
    "rangetoplot = np.linspace(0, 1, 100)  # 0 to 1\n",
    "\n",
    "graph.plot(rangetoplot, prior.pdf(rangetoplot, 1, 1))\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update our prior, based on observed data. Again, let's not worry about the math so much as the idea that we can update our prior probability into a posterior, by seeing the oberved values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[1 0 1 0 0 0 1 0 0 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "n = 10 # number of samples\n",
    "\n",
    "# data = stats.bernoulli.rvs(0.5, size=n) # samples\n",
    "# heads = data.sum()\n",
    "# y = prior(1 + heads, 1 + n - heads).pdf(rangetoplot)\n",
    "# print(data)\n",
    "\n",
    "data2 = stats.bernoulli.rvs(0.5, size=n)\n",
    "successes = data2.sum()\n",
    "print(successes)\n",
    "z = prior(2 + successes, 9 + n - successes).pdf(rangetoplot)\n",
    "print(data2)\n",
    "\n",
    "graph.plot(rangetoplot, z, label=f'observed {n} tosses,\\n {heads} heads')\n",
    "graph.show()\n",
    "print(heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is now our posterior distribution - this is how likely we think we are to get heads coming forwards. Note it is technically not an estimate of the value, but of our belief in the value.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "1. What happens as we increase N? run N for `[1,5,10,15,50,200]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension, Optional - Bayesian inference with PyMC3\n",
    "\n",
    "PyMC3 is the current state of art in Bayesian inference in python. Once we have more complicated models (you can imagine if we had something similar to the linear regressions we have run in the last week), the math becomes even more intractable.\n",
    "\n",
    "PyMC3 relies on us to tell it the expected priors and distributions of individual parameters, and then uses advanced math and sampling to calculate our priors by simulation. You can imagine it as simulating the data via our distributions, and then inferring the posterior from them. The key here is creating the model - priors and parameters rely on how they are specified. The great part of PyMC3 is that as long as you have an idea of your model, you don't need the math to calculate anything!\n",
    "\n",
    "A great resource on PyMC3 is the textbook ['Probabilistic Programming and Bayesian Methods for Hackers'](http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/tree/master/), which this example is modified from.\n",
    "\n",
    "Let's write our coin example in PyMC3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import scipy.stats as stats\n",
    "\n",
    "n = 100\n",
    "\n",
    "data = stats.bernoulli.rvs(0.5, size=n) # samples\n",
    "heads = data.sum()\n",
    "\n",
    "# priors of the distribution\n",
    "alpha = 1 # prior of alpha\n",
    "beta = 1 # prior of beta\n",
    "\n",
    "n_samples = 1000  # number of times to simulate our data\n",
    "\n",
    "with pm.Model() as model:  # context management\n",
    "    # Priors\n",
    "    prior = pm.Beta('p', alpha=alpha, beta=beta)\n",
    "\n",
    "    # Likelihood\n",
    "    y = pm.Binomial('y', n=n, p=prior, observed=heads)\n",
    "\n",
    "    # Sample posterior\n",
    "    trace = pm.sample(n_samples)\n",
    "    \n",
    "    pm.traceplot(trace)\n",
    "    graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the backend, this is comping the core math to C and running it, so it may take a while to run!\n",
    "\n",
    "We can plot our outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 100)\n",
    "\n",
    "graph.hist(trace['p'], 15, histtype='step', normed=True, label='post')\n",
    "graph.plot(x, stats.beta.pdf(x, alpha, beta), label='prior')\n",
    "graph.legend(loc='best')\n",
    "graph.show()\n",
    "\n",
    "# Or far more conveniently\n",
    "pm.plot_posterior(trace)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Try changing the parameters of the prior - how does this change the posterior?\n",
    "2. Try changing N (not too large!) - how does this change the posterior?\n",
    "3. Try chaning the coin to be biased - do we get a better or worse posterior in the same number of samples?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
